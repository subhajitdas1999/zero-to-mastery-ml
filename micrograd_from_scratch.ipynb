{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5faf0ac-1cb3-4ee3-b52c-83118752969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86577e5b-1e24-4fc3-94d3-ea2aba73b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749112e3-8033-410f-80aa-71497be10388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3878580-2b9a-45ea-91ad-8994ad9d23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5,5,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83cfb79-f07f-4fb5-842d-dd84d1a1918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.   -4.75 -4.5  -4.25 -4.   -3.75 -3.5  -3.25 -3.   -2.75 -2.5  -2.25\n",
      " -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "  1.    1.25  1.5   1.75  2.    2.25  2.5   2.75  3.    3.25  3.5   3.75\n",
      "  4.    4.25  4.5   4.75]\n"
     ]
    }
   ],
   "source": [
    "print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04077f99-2c4c-4449-89d1-f747073379b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.      91.6875  83.75    76.1875  69.      62.1875  55.75    49.6875\n",
      "  44.      38.6875  33.75    29.1875  25.      21.1875  17.75    14.6875\n",
      "  12.       9.6875   7.75     6.1875   5.       4.1875   3.75     3.6875\n",
      "   4.       4.6875   5.75     7.1875   9.      11.1875  13.75    16.6875\n",
      "  20.      23.6875  27.75    32.1875  37.      42.1875  47.75    53.6875]\n"
     ]
    }
   ],
   "source": [
    " ys = f(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee68adf-c514-4c7a-b513-88b8f41e03f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7a9443e2e020>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5429469c-bc0c-413b-a818-8a4356000b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.000270000025239"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=0.00009\n",
    "a = 3\n",
    "(f(a+h)-f(a))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d9acfe8-75ca-4b0c-85c4-3876384c1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self,data, _children=(),_op=\"\",label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda : None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data} label={self.label})\"\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # so that i can add a numerical also\n",
    "        out = Value(self.data + other.data,(self,other),\"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad # for multiple edges (same Value contributing multiple places) the grad should get accumulate\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "        \n",
    "    def __sub__(self,other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self,other):\n",
    "        return other + (-self)\n",
    "        \n",
    "    def __radd__(self,other): #so that we can call 1 + a(value object) it's a fallback\n",
    "        return self + other\n",
    "        \n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data,(self,other),\"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self,other):\n",
    "        assert isinstance(other,(int,float)),\"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other,(self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += ((other * (self.data)**(other-1))) * (out.grad)\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __rmul__(self,other): #so that we can call 1 * a(value object)\n",
    "        return self*other\n",
    "        \n",
    "    def __truediv__(self,other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "        \n",
    "    def tanh(self):\n",
    "        n = self.data\n",
    "        x = (math.exp(2*n) - 1) / (math.exp(2*n) + 1)\n",
    "        out = Value(x,(self,),\"tanh\")\n",
    "        def _backward():\n",
    "            self.grad += ((1 - x**2) * (out.grad))\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo=[]\n",
    "        visited=set()\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(node)\n",
    "        \n",
    "        \n",
    "        build_topo(self)\n",
    "        self.grad=1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x),(self,),\"exp\")\n",
    "        def _backward():\n",
    "            self.grad += ((math.exp(x)) * (out.grad))\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "# a = Value(2.0,label=\"a\")\n",
    "# b = Value(-3.0, label=\"b\")\n",
    "# c = Value(10.0, label=\"c\")\n",
    "# e = a * b ; e.label=\"e\"\n",
    "# d= e + c; d.label=\"d\"\n",
    "# f = Value(-2.0,label=\"f\")\n",
    "# L = d * f; L.label=\"L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c66ac827-b32e-411f-b32f-cf9317e3d925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.0 label=)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(4.0,label=\"a\")\n",
    "b = Value(2.0,label=\"b\")\n",
    "\n",
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ec5f8e-2036-45d6-bee7-9852d7c259b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acdb0706-f830-46c1-9b88-1bd0d4dedaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    " \n",
    "    dot = Digraph(format=\"svg\", graph_attr={'rankdir': 'LR'}) # LR = Left to Right\n",
    "    nodes, edges = trace(root)\n",
    "    \n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name=uid, label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n",
    "\n",
    "        if n._op:\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            dot.edge(uid + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0207be2f-0bff-4d86-a920-79fe57584af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972a72e-caf1-496b-9eec-5ca1099a10b9",
   "metadata": {},
   "source": [
    "### time to fill up the grad and do back propagation manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e655bca-a896-41ee-9dc6-ee063fdec2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for L node\n",
    "# derivative of L w.r.t L (if I change L by a tiny amount of h how much does L change)\n",
    "# dL/ dL = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e7b3cc97-654c-4711-995d-34ac8e9a1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L.grad = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbead629-75f9-4f00-a53e-5ad2844659ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the node d and f\n",
    "# dL/dd = ? and dL/df ?\n",
    "# L = d*f =\n",
    "#  dL/ dd = f = -2.0 and dL/df = d = 4.0\n",
    "d.grad = -2.0\n",
    "f.grad = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b8183-9844-4960-8852-9d2954dcd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the node c and e\n",
    "# d = c + e\n",
    "# dL/dc and dL/de\n",
    "# using chain rule by looking at the equation \n",
    "# dL/dc = dL/dd x dd/dc (dL/dd = -2.0 from previous step)\n",
    "#       = -2.0 x d/dc (c + e) = -2.0 x 1 = -2.0\n",
    "# dL/dc = 12.0\n",
    "# dL/de = dL/dd x dd/de = -2.0 x 1 = -2.0 \n",
    "c.grad = -2.0\n",
    "e.grad = -2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247496e3-b204-455c-b884-b494518d7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculating for a , b\n",
    "# a * b = e\n",
    "# dL/da = ? and dL/db = ?\n",
    "# again using a chain rule now\n",
    "# by looking at the equvalent equation\n",
    "# dL/da = dL/de * de/da = -2.0 * d/da (ab) = -2.0 * b = -2.0 * -3.0 = 6\n",
    "# dL/db = dL/de * de/db = -2.0 * d/db (ab) = -2.0 * a = -2.0 * 2.0 = -4.0\n",
    "a.grad = 6.0\n",
    "b.grad = -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7fce6-c31c-4849-8364-b6274896f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the derivative numericals manually\n",
    "\n",
    "def lol():\n",
    "\n",
    "    h= 0.001\n",
    "    \n",
    "    a = Value(2.0,label=\"a\")\n",
    "    b = Value(-3.0, label=\"b\")\n",
    "    c = Value(10.0, label=\"c\")\n",
    "    e = a * b ; e.label=\"e\"\n",
    "    d= e + c; d.label=\"d\"\n",
    "    f = Value(-2.0,label=\"f\")\n",
    "    L = d * f; L.label=\"L\"\n",
    "    L1 = L.data\n",
    "\n",
    "    a = Value(2.0,label=\"a\")\n",
    "    b = Value(-3.0 + h, label=\"b\")\n",
    "    c = Value(10.0, label=\"c\")\n",
    "    e = a * b ; e.label=\"e\"\n",
    "    d= e + c; d.label=\"d\"\n",
    "    f = Value(-2.0,label=\"f\")\n",
    "    L = d * f; L.label=\"L\"\n",
    "    L2 = L.data\n",
    "\n",
    "    print((L2 - L1) / h)\n",
    "\n",
    "lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480f0be-b27a-48d3-8635-e096371fc03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.tanh(-19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620acb04-6bad-4426-8d8a-580b1d4ed1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(-5,5,0.2),np.tanh(np.arange(-5,5,0.2)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcc263-2976-4431-b1d3-612c7692d444",
   "metadata": {},
   "source": [
    "## Now I am gonna implement a small nueron , and use tanh as a squash func\n",
    "![nueron](https://cs231n.github.io/assets/nn1/neuron_model.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496def8-a1f0-4aa0-b764-2db51db9244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96bca1-9691-434a-bad9-6c222ffa3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add grad now\n",
    "o.grad = 1.0 #derivative of o w.r.t o is 1\n",
    "\n",
    "# o= tanh(n) => d (tanh(x))/ dx = 1- tanh^2(x) => d (tanh(n)) / dn => 1-tanh^2(n) => 1- o^2 (as o = tanh(n))\n",
    "n.grad =  1- (o.data)**2 #0.5\n",
    "\n",
    "b.grad = 0.5 # do/db = do/dn x dn/db = 0.5 x 1\n",
    "\n",
    "x1w1x2w2.grad = 0.5\n",
    "x1w1.grad = 0.5 * 1.0\n",
    "x2w2.grad = 0.5 * 1.0\n",
    "x1.grad = 0.5 * w1.data\n",
    "w1.grad = 0.5 * x1.data\n",
    "x2.grad = 0.5 * w2.data\n",
    "w2.grad = 0.5 * x2.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b33417-57f4-40f3-9153-bdccf7081216",
   "metadata": {},
   "source": [
    "### Now try to implement this grad automatically observing the behaviour of plus and mul and tanh. \n",
    "#### in addtion it will be 1 * (forward local grad)\n",
    "### in mul it will be other.data x (forward local grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a5071-2bcd-49b4-8839-b60eff1bf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way of automatically adding grad\n",
    "o.grad = 1.0 # base condition\n",
    "o._backward()\n",
    "n._backward()\n",
    "x1w1x2w2._backward()\n",
    "x1w1._backward()\n",
    "x2w2._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730179e-9660-4f42-ab08-4a2fd462b92b",
   "metadata": {},
   "source": [
    "### Now Try to automate this using topological sort so that we dont have to call backward function manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75986fa9-b755-47d6-98af-90fa78e915ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs variables\n",
    "x1 = Value(2.0,label=\"x1\")\n",
    "x2 = Value(0,label=\"x2\")\n",
    "\n",
    "# weights \n",
    "w1 = Value(-3.0,label=\"w1\")\n",
    "w2 = Value(1.0,label=\"w2\")\n",
    "\n",
    "# bias\n",
    "b = Value(6.8813735870195432,label=\"b\")\n",
    "x1w1=x1 * w1; x1w1.label = \"x1w1\"\n",
    "x2w2 = x2 * w2;x2w2.label = \"x2w2\"\n",
    "x1w1x2w2 = x1w1 + x2w2 ; x1w1x2w2.label = \"x1w1 + x2w2\"\n",
    "\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "o = n.tanh();o.label=\"o\"\n",
    "\n",
    "# ------- Now update the o as we have multiple math opearation in value class\n",
    "# e = (2*n).exp(); e.label=\"e\"\n",
    "# o = (e+1) / (e-1)\n",
    "# o.label = \"o\"\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5c15b-e318-42d5-9f04-2c4784eeb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82e1bb-76ff-44d5-9860-9432df32d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topological sort\n",
    "# topo=[]\n",
    "# visited=set()\n",
    "# def build_topo(node):\n",
    "#     if node not in visited:\n",
    "#         visited.add(node)\n",
    "#         for child in node._prev:\n",
    "#             build_topo(child)\n",
    "#         topo.append(node)\n",
    "\n",
    "\n",
    "# build_topo(o)\n",
    "# o.grad=1.0\n",
    "# for node in reversed(topo):\n",
    "#     node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b3697-779d-40d7-bc9b-a87185044981",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b3ebd-933f-48af-8287-6de2ba6d4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0,label=\"a\")\n",
    "1 + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f9d2a-8edb-436d-a2c4-4d5259f25a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(4.0,label=\"a\")\n",
    "2.6 * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253f8f0-064d-456d-909d-0c08505e9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs variables\n",
    "x1 = Value(2.0,label=\"x1\")\n",
    "x2 = Value(0,label=\"x2\")\n",
    "\n",
    "# weights \n",
    "w1 = Value(-3.0,label=\"w1\")\n",
    "w2 = Value(1.0,label=\"w2\")\n",
    "\n",
    "# bias\n",
    "b = Value(6.8813735870195432,label=\"b\")\n",
    "x1w1=x1 * w1; x1w1.label = \"x1w1\"\n",
    "x2w2 = x2 * w2;x2w2.label = \"x2w2\"\n",
    "x1w1x2w2 = x1w1 + x2w2 ; x1w1x2w2.label = \"x1w1 + x2w2\"\n",
    "\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "o = n.tanh();o.label=\"o\"\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95e360-6983-442d-986d-aba102bd761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs variables\n",
    "x1 = Value(2.0,label=\"x1\")\n",
    "x2 = Value(0,label=\"x2\")\n",
    "\n",
    "# weights \n",
    "w1 = Value(-3.0,label=\"w1\")\n",
    "w2 = Value(1.0,label=\"w2\")\n",
    "\n",
    "# bias\n",
    "b = Value(6.8813735870195432,label=\"b\")\n",
    "x1w1=x1 * w1; x1w1.label = \"x1w1\"\n",
    "x2w2 = x2 * w2;x2w2.label = \"x2w2\"\n",
    "x1w1x2w2 = x1w1 + x2w2 ; x1w1x2w2.label = \"x1w1 + x2w2\"\n",
    "\n",
    "n = x1w1x2w2 + b; n.label=\"n\"\n",
    "\n",
    "# o = n.tanh();o.label=\"o\"\n",
    "\n",
    "# ------- Now update the o as we have multiple math opearation in value class\n",
    "e = (2*n).exp(); e.label=\"e\"\n",
    "o = (e-1) / (e+1)\n",
    "o.label = \"o\"\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd1fe2-0612-4a73-ab4a-fddf2cbbc69f",
   "metadata": {},
   "source": [
    "### Defined Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9fdf0-4d38-43d6-a869-6efbef9006f5",
   "metadata": {},
   "source": [
    "## Now I am gonna implement a nueron network , and use tanh as a squash func\n",
    "![nueron network](https://cs231n.github.io/assets/nn1/neural_net2.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6c9d061-5ea2-421f-8b31-4df84ceabd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.8927930063628413 label=)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,nin): #nin = number of inputs in the nueron\n",
    "        # here assingning the weights if the inputs\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b =Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self,x):\n",
    "            # x*w + b\n",
    "            \n",
    "            act = sum(xi*wi for xi,wi in zip(x,self.w)) + self.b\n",
    "            out = act.tanh()\n",
    "            \n",
    "            return out\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "# n = Neuron(2)\n",
    "x = [2.0,3.0]\n",
    "# n(x) # This will invoke the __call__ method\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self,nin,nout):\n",
    "        # print(\"Layer: initialised with \",nin,nout)\n",
    "        self.neurons =[Neuron(nin) for _ in range(nout)] #This will be like a layer for example here 3 neurons with 2 inputs and 1 output\n",
    "\n",
    "    def __call__(self,x):\n",
    "        outs = [n(x)for n in self.neurons]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for nueron in self.neurons:\n",
    "            params.extend(nueron.parameters())\n",
    "        return params\n",
    "# n = Layer(2,3) #2,3 means we have total 3 neurons in a layer and each nueron has 2 input and 1 output\n",
    "# n(x)\n",
    "\n",
    "class MLP: #Multilayer Perceptron (MLP)\n",
    "    def __init__(self,nin,nouts):\n",
    "        sz = [nin]+nouts\n",
    "        self.layers =[Layer(sz[i],sz[i+1]) for i in range(len(nouts))]\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            # print(\"MLP: Layer called with input\",x)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            for p in layer.parameters():\n",
    "                params.append(p)\n",
    "        return params\n",
    "\n",
    "x = [2.0,3.0,-1.0]\n",
    "n = MLP(3,[4,4,1]) #This is trying to simulate the above MLP in diagram considering \n",
    "# first layer having 3 (consider it as a inputs) and rest defined number of neurons needs to be present at each layer stating \n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b1fe6-fdab-416d-9cba-ffce3fc8ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15d45b36-b516-46ee-8e42-9473513f7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "ys = [1.0,-1.0,-1.0,1.0] #desired targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbdbc438-e318-4d05-957b-04f89e33fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP: Layer called with input [2.0, 3.0, -1.0]\n",
      "MLP: Layer called with input [Value(data=-0.9008236202884149 label=), Value(data=0.9995140087597716 label=), Value(data=-0.9950213896211352 label=), Value(data=0.9999661488438927 label=)]\n",
      "MLP: Layer called with input [Value(data=0.22416780310724227 label=), Value(data=-0.7657957584736903 label=), Value(data=-0.962774259284375 label=), Value(data=0.940706513232283 label=)]\n",
      "MLP: Layer called with input [3.0, -1.0, 0.5]\n",
      "MLP: Layer called with input [Value(data=0.9959933807500115 label=), Value(data=-0.919916064718418 label=), Value(data=-0.722287380651278 label=), Value(data=0.9970516329432914 label=)]\n",
      "MLP: Layer called with input [Value(data=0.9712362747107609 label=), Value(data=0.9654785688881515 label=), Value(data=0.8864049691678334 label=), Value(data=-0.9995572359406973 label=)]\n",
      "MLP: Layer called with input [0.5, 1.0, 1.0]\n",
      "MLP: Layer called with input [Value(data=0.9835150464702078 label=), Value(data=-0.647196804349521 label=), Value(data=-0.6211966360789213 label=), Value(data=0.9554596646165491 label=)]\n",
      "MLP: Layer called with input [Value(data=0.9715203925867595 label=), Value(data=0.9610709913019168 label=), Value(data=0.8455413240258712 label=), Value(data=-0.99876937741892 label=)]\n",
      "MLP: Layer called with input [1.0, 1.0, -1.0]\n",
      "MLP: Layer called with input [Value(data=-0.7926168607033929 label=), Value(data=0.9758700033030346 label=), Value(data=-0.9595723716158413 label=), Value(data=0.9974835530840556 label=)]\n",
      "MLP: Layer called with input [Value(data=0.34202661667640927 label=), Value(data=-0.6903425911265135 label=), Value(data=-0.9520146728737789 label=), Value(data=0.9127789336555312 label=)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9916299758014671 label=),\n",
       " Value(data=-0.9994584618317435 label=),\n",
       " Value(data=-0.9994315982467912 label=),\n",
       " Value(data=0.9859949988186446 label=)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs] #currently the output we are getting\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f3aca-1d79-40a5-bd3f-4d0432d11569",
   "metadata": {},
   "source": [
    "### Calculating the loss\n",
    "### we are going to implement mean sqare error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "deb8747c-c995-40b8-9f90-e4e70c2b8155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=7.005730508402705e-05 label=), Value(data=2.932635876786355e-07 label=), Value(data=3.2308055305086576e-07 label=), Value(data=0.00019614005808976594 label=)]\n",
      "Value(data=0.00026681370731452246 label=)\n"
     ]
    }
   ],
   "source": [
    "loss = [(ygt-yout)**2 for ygt,yout in zip(ys,ypred)] #This is going to calculate loss for each 4 inputs\n",
    "print(loss)\n",
    "loss = sum(loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f051ed-d54a-4274-997d-274d2748f479",
   "metadata": {},
   "source": [
    "### If we are way off the loss would be high , so we want low loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8902a-16c1-4db2-abf3-88febdc2768e",
   "metadata": {},
   "source": [
    "### Now lets sum the loss and our only target will be to reduce the overall loss (loss minimum is 0 and greater it is our NN is worst predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "405d3ef0-37a8-4e19-b702-475b203f6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neurons[0].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1eb04e28-1e74-4220-a599-27e78cc19fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before calling the backword pass we need to reset the grad for that parameters that we going to update\n",
    "    # this is because gard is getting accumulated , so we need to flush the grad and reset it to 1 befor calculating\n",
    "    # the backword pass which is going add the update grads in the nodes\n",
    "    # other wise old grad stays and new grad is getting accumulated on top if it\n",
    "for p in n.parameters():\n",
    "    p.grad = 0.0\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75f3c4-28e9-4d36-8dd5-4547bba37e33",
   "metadata": {},
   "source": [
    "### If grad is +ve means if we increse the value of this node the loss will also increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a05592-7423-4877-b5a8-66e96fb57b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c122f38-8413-411c-87b2-1eaf3cef184e",
   "metadata": {},
   "source": [
    "### Added paramerts details in the NN classes . Because as we be reducing the lossing by nudjing the input . Now the X values are inputs which are fixed we can adjust the weights and bias in that case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e53c679e-3519-4c02-a9b6-4f4c8c05a8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.23869878725569593 label=),\n",
       " Value(data=-0.5366467953674388 label=),\n",
       " Value(data=0.8602558171287449 label=),\n",
       " Value(data=0.27561452011659066 label=),\n",
       " Value(data=-0.07106458165702023 label=),\n",
       " Value(data=0.6819712644897198 label=),\n",
       " Value(data=-0.7616016463638879 label=),\n",
       " Value(data=-0.19825072110359157 label=),\n",
       " Value(data=-0.3370367676146474 label=),\n",
       " Value(data=-0.39505167858933077 label=),\n",
       " Value(data=0.5295239848955053 label=),\n",
       " Value(data=-0.7682514043919179 label=),\n",
       " Value(data=0.5619972795672841 label=),\n",
       " Value(data=0.0863271298432089 label=),\n",
       " Value(data=-0.25110579288052376 label=),\n",
       " Value(data=1.0699250464121801 label=),\n",
       " Value(data=0.9099963686947353 label=),\n",
       " Value(data=0.6988623230212508 label=),\n",
       " Value(data=0.07477985707634158 label=),\n",
       " Value(data=0.423982165253425 label=),\n",
       " Value(data=-0.6732284443254578 label=),\n",
       " Value(data=0.9190863953076774 label=),\n",
       " Value(data=0.14088649949865806 label=),\n",
       " Value(data=-0.4697918875027235 label=),\n",
       " Value(data=-0.2925813246569414 label=),\n",
       " Value(data=0.3914726354559765 label=),\n",
       " Value(data=0.3018425625832868 label=),\n",
       " Value(data=-0.714511753521276 label=),\n",
       " Value(data=0.6242451401886209 label=),\n",
       " Value(data=-0.8842144349657977 label=),\n",
       " Value(data=0.813921095712163 label=),\n",
       " Value(data=-0.6210870223765824 label=),\n",
       " Value(data=0.9236387170353104 label=),\n",
       " Value(data=-0.07519577279945258 label=),\n",
       " Value(data=-0.3957977861711876 label=),\n",
       " Value(data=0.8095834047524962 label=),\n",
       " Value(data=-0.7423518296259298 label=),\n",
       " Value(data=-0.6141660524397967 label=),\n",
       " Value(data=-0.3071079088992867 label=),\n",
       " Value(data=1.1096015913597446 label=),\n",
       " Value(data=0.44934414522444927 label=)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d539358-de12-4c6b-8057-a3f0e888d0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fdb31-419a-4dde-aa08-3e6693a9b8b2",
   "metadata": {},
   "source": [
    "### Now As we have the parameter details of our NN Now to improve our NN prediction we will try to adjust the parameter details by studying the grad of a parameter. If the grad is -ve means increasing that data will decrease the loss and for +ve decreasing thar velue will decrease the loss value\n",
    "\n",
    "### will try to nudje this by a vary small factor , will update the data of params, forward pass (the prediction), backword pass , check loss, check output , repeat until get get our desired output.\n",
    "\n",
    "### we change the data by a very large factor then we might missed or jump from a actual point so we will do it by a small factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b42ffda3-51d1-4639-939e-62b547af66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.01 * p.grad\n",
    "    #0.01 is tge factor we are using and multiplying with it's grad . If grad is -ve the we will increase the p.data\n",
    "    # value , if grad is +ve the we will decrease the p.data value to improve/reduce the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8619526-27a0-4bad-814b-2eaf0a3e768c",
   "metadata": {},
   "source": [
    "### Just a update :- after repeatating doing (alomost 10-15 times) the will update the data of params (above step), forward pass (the prediction), backword pass of loss, check loss, check output , repeat until get get our desired output.\n",
    "\n",
    "### I am getting\n",
    "\n",
    "### sum_loss = Value(data=0.00026681370731452246 label=)\n",
    "### output = [Value(data=0.9916299758014671 label=),\n",
    "### Value(data=-0.9994584618317435 label=),\n",
    "### Value(data=-0.9994315982467912 label=),\n",
    "### Value(data=0.9859949988186446 label=)]\n",
    "\n",
    "### which is alomost matiching out desired output [1.0,-1.0,-1.0,1.0]\n",
    "\n",
    "\n",
    "\n",
    "### Below I am accumulating all the steps in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f5373-8ca0-469c-b2e0-318321e6962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just accumulating this septs together also\n",
    "n = MLP(3,[4,4,1]) # nn\n",
    "# inputs\n",
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "ys = [1.0,-1.0,-1.0,1.0] #desired targets\n",
    "ypred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cca74e27-e590-4563-8e83-10729084d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Value(data=0.00010971884783576814 label=)\n",
      "1 Value(data=0.00010914930111831838 label=)\n",
      "2 Value(data=0.00010858557985609301 label=)\n",
      "3 Value(data=0.00010802759547124167 label=)\n",
      "4 Value(data=0.00010747526116912858 label=)\n",
      "5 Value(data=0.00010692849189374205 label=)\n",
      "6 Value(data=0.00010638720428442324 label=)\n",
      "7 Value(data=0.00010585131663389146 label=)\n",
      "8 Value(data=0.00010532074884748572 label=)\n",
      "9 Value(data=0.00010479542240363723 label=)\n",
      "10 Value(data=0.00010427526031547518 label=)\n",
      "11 Value(data=0.0001037601870935574 label=)\n",
      "12 Value(data=0.00010325012870967694 label=)\n",
      "13 Value(data=0.0001027450125617264 label=)\n",
      "14 Value(data=0.00010224476743956283 label=)\n",
      "15 Value(data=0.00010174932349182495 label=)\n",
      "16 Value(data=0.00010125861219374132 label=)\n",
      "17 Value(data=0.00010077256631580677 label=)\n",
      "18 Value(data=0.00010029111989336536 label=)\n",
      "19 Value(data=9.934176770393891e-05 label=)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for step in range(20):\n",
    "\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(ygt-yout)**2 for ygt,yout in zip(ys,ypred)])\n",
    "\n",
    "    # before calling the backword pass we need to reset the grad for that parameters that we going to update\n",
    "    # this is because gard is getting accumulated , so we need to flush the grad and reset it to 1 befor calculating\n",
    "    # the backword pass which is going add the update grads in the nodes\n",
    "    # other wise old grad stays and new grad is getting accumulated on top if it\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    # backword pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update the params\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "\n",
    "    # edge case for the last round\n",
    "    if step == 19:\n",
    "        # forward pass\n",
    "        ypred = [n(x) for x in xs]\n",
    "        loss = sum([(ygt-yout)**2 for ygt,yout in zip(ys,ypred)])\n",
    "    \n",
    "        # backword pass\n",
    "        loss.backward()\n",
    "        \n",
    "    print(step, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "237025f7-d3ad-4076-9066-1d0075450b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.995526843332068 label=),\n",
       " Value(data=-0.9967037769414128 label=),\n",
       " Value(data=-0.9943248049644591 label=),\n",
       " Value(data=0.9939783962281894 label=)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b337abe-9e5e-49c0-8e4f-737c4c412a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
